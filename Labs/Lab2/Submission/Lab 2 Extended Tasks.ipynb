{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2: Extended Tasks Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Method to parse the structure of an html page using package beautifulsoup.\n",
    "# The code looks for specific tags in the html structure and extracts the content\n",
    "def getArticleDetailsByUrl(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "    #soup.prettify()\n",
    "    \n",
    "    headline = soup.title.string\n",
    "    subheadline = soup.head.find(\"meta\",attrs={\"name\":\"description\"}).get('content')\n",
    "\n",
    "    doc_body = ''\n",
    "    if \"The Irish Times\" in soup.text:\n",
    "        for body_p_tag in soup.article.find_all(\"p\", attrs={\"class\": \"no_name\"}):\n",
    "            doc_body += body_p_tag.get_text() + \" \"\n",
    "\n",
    "    source = \"Other\"\n",
    "    try:\n",
    "        if \"irishtimes\" in url:\n",
    "            source = \"IrishTimes\"\n",
    "            body_p_tag = soup.article.find(\"div\", attrs={\"class\": \"last_updated\"}).find(\"p\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    first_sentence = doc_body.split(\".\")[0]\n",
    "\n",
    "    return [headline, subheadline, first_sentence, doc_body, source]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior entrepreneurs encouraged to start real ...</td>\n",
       "      <td>A few years ago entrepreneur and multimilliona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New personal injury measures needed to reduce ...</td>\n",
       "      <td>A report has called for new guidelines for jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Councils should buy vacant houses compulsorily...</td>\n",
       "      <td>A major programme of compulsory purchasing of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unemployment main driver for Spaniards moving ...</td>\n",
       "      <td>When Alvaro Cabello came to Ireland in 2012 it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Surgical mesh failure rates unacceptably high,...</td>\n",
       "      <td>Failure rates in some treatments using “surgi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Man (83) ‘hadn’t much choice’ but to try stop ...</td>\n",
       "      <td>An 83-year-old man who helped to thwart an att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Minister cautions that report on future of pol...</td>\n",
       "      <td>Minister for Justice Charlie Flanagan has said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cervical cancer vaccine uptake rises to 65%</td>\n",
       "      <td>The rate of uptake of the cervical cancer vacc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lock up your tractors, gardaí warn, as one in ...</td>\n",
       "      <td>Organised crime gangs are targeting farm equip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Woman (31) dies after being stabbed in Dundalk</td>\n",
       "      <td>A woman has died following a stabbing incident...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        Title  \\\n",
       "Article_ID                                                      \n",
       "1           Junior entrepreneurs encouraged to start real ...   \n",
       "2           New personal injury measures needed to reduce ...   \n",
       "3           Councils should buy vacant houses compulsorily...   \n",
       "4           Unemployment main driver for Spaniards moving ...   \n",
       "5           Surgical mesh failure rates unacceptably high,...   \n",
       "6           Man (83) ‘hadn’t much choice’ but to try stop ...   \n",
       "7           Minister cautions that report on future of pol...   \n",
       "8                 Cervical cancer vaccine uptake rises to 65%   \n",
       "9           Lock up your tractors, gardaí warn, as one in ...   \n",
       "10             Woman (31) dies after being stabbed in Dundalk   \n",
       "\n",
       "                                                      Content  \n",
       "Article_ID                                                     \n",
       "1           A few years ago entrepreneur and multimilliona...  \n",
       "2           A report has called for new guidelines for jud...  \n",
       "3            A major programme of compulsory purchasing of...  \n",
       "4           When Alvaro Cabello came to Ireland in 2012 it...  \n",
       "5            Failure rates in some treatments using “surgi...  \n",
       "6           An 83-year-old man who helped to thwart an att...  \n",
       "7           Minister for Justice Charlie Flanagan has said...  \n",
       "8           The rate of uptake of the cervical cancer vacc...  \n",
       "9           Organised crime gangs are targeting farm equip...  \n",
       "10          A woman has died following a stabbing incident...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "\n",
    "d = feedparser.parse('http://www.irishtimes.com/cmlink/news-1.1319192')\n",
    "        \n",
    "i = int\n",
    "i = 1\n",
    "\n",
    "df = pd.DataFrame({'Article_ID': [], 'Title': [], 'Content': []})\n",
    "    \n",
    "for item in d['entries']:\n",
    "    article_url = item['link']\n",
    "    [ headline, subheadline, first_sentence, doc_body, source] = getArticleDetailsByUrl(article_url) \n",
    "    df = df.append({'Article_ID': int(i), 'Title': headline, 'Content': doc_body}, ignore_index=True)\n",
    "    i+=1\n",
    "    \n",
    "df['Article_ID'] = df.Article_ID.astype(int)\n",
    "\n",
    "df.set_index('Article_ID', inplace=True)\n",
    "    \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2: Extended Tasks Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Twitter Search API to get public tweets from the past\n",
    "# Initiate the connection to Twitter REST API\n",
    "import json\n",
    "# Import the necessary methods from \"twitter\" library\n",
    "# Please make sure twitter is installed: pip install twitter\n",
    "from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream\n",
    "\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "# ACCESS_TOKEN = 'YOUR ACCESS TOKEN\"'\n",
    "# ACCESS_SECRET = 'YOUR ACCESS TOKEN SECRET'\n",
    "# CONSUMER_KEY = 'YOUR API KEY'\n",
    "# CONSUMER_SECRET = 'ENTER YOUR API SECRET'\n",
    "ACCESS_TOKEN = '2839893905-pBXUzdrHCNXyjfPuBpSwxNbH1zyEpRaa2sXK0Jd'\n",
    "ACCESS_SECRET = 'eNtB7YTAfsMhPIQtKji8aQT7zQFpFfDPR2lQ89WKfgI1U'\n",
    "CONSUMER_KEY = 'ZqPrfLpc0znZlz3kW2a22VmUa'\n",
    "CONSUMER_SECRET = 'BHD19T0DmUV2XVvEhUAgvpXMx0nGfxevAtr53NbCd9jQjPyTqn'\n",
    "\n",
    "oauth = OAuth(ACCESS_TOKEN, ACCESS_SECRET, CONSUMER_KEY, CONSUMER_SECRET)\n",
    "twitter = Twitter(auth=oauth)\n",
    "            \n",
    "# Search for latest 100 tweets about \"#analytics\"\n",
    "iterator = twitter.search.tweets(q='#analytics', result_type='recent', lang='en', count=100)\n",
    "#print(json.dumps(iterator, indent=4))\n",
    "\n",
    "file = open(\"twitter_search_100tweets.json\", \"w\") \n",
    "for tweet in iterator['statuses']:\n",
    "    #print(json.dumps(tweet))\n",
    "    file.write(json.dumps(tweet)+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n",
      "JSON error!!!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# We use the file saved from last step as example\n",
    "with open('twitter_search_100tweets.json', 'r') as f:\n",
    "    tweets_file = f.readlines()\n",
    "#print(tweets_file)\n",
    "\n",
    "df2 = pd.DataFrame({'User_ID': [], 'Text': [], 'No_Retweet': [], 'User_Retweet': []})\n",
    "\n",
    "for line in tweets_file:\n",
    "    #print(line)\n",
    "    try:\n",
    "        # Read in one line of the file, convert it into a json object \n",
    "        tweet = json.loads(line.strip())\n",
    "        #print(tweet)\n",
    "        if 'text' in tweet:\n",
    "            id = tweet['id']\n",
    "            text = tweet['text']\n",
    "            rtcount = tweet['retweet_count']\n",
    "            try: \n",
    "                userrt = tweet['retweeted_status']['in_reply_to_user_id']\n",
    "            except:\n",
    "                userrt = None \n",
    "            \n",
    "            df2 = df2.append({'User_ID': id, 'Text': text, 'No_Retweet': rtcount, 'User_Retweet': userrt}, ignore_index=True)\n",
    "                \n",
    "            df2['User_ID'] = df2.User_ID.astype(int)\n",
    "\n",
    "            df2.set_index('User_ID', inplace=True)\n",
    "    \n",
    "            df2.head(10)\n",
    "    \n",
    "    except:\n",
    "        # read in a line that is not in JSON format (sometimes error occured)\n",
    "        print(\"JSON error!!!\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
